# Experiment Title: Implement text document similarity recognizer for the given text
# Experiment No: 03
# Author: Adrija Dastidar
# PRN: 1032221315
# Import Libraries
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('punkt_tab')
# Sample Documents
documents = [
    "This is a sample document for testing the text similarity",
    "We will use NLTK for computing similarity of text",
    "NLTK is a powerful library for natural language processing"
]
# Define stop words and stemmer
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
def preprocess_text(text):
    """Tokenizes, removes stopwords, and applies stemming."""
    words = word_tokenize(text.lower())
    filtered_words = []

    for word in words:
        if word.isalnum() and word not in stop_words:
            filtered_words.append(stemmer.stem(word))

    return ' '.join(filtered_words)
# Preprocess documents
preprocessed_docs = []

for doc in documents:
    preprocessed_docs.append(preprocess_text(doc))
# Display preprocessed documents
print("Preprocessed Documents:")
for i in range(len(documents)):
    print(f"Document {i+1}: {preprocess_text(documents[i])}")
# TF-IDF
tfidf_vectorizer = TfidfVectorizer()
tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_docs)
feature_names = tfidf_vectorizer.get_feature_names_out()
# Display TF-IDF matrix
print("TF-IDF Matrix:")
for i in range(len(documents)):
    print(f"Document {i+1}:")
    for j in range(len(feature_names)):
        print(f"\t {feature_names[j]}: {tfidf_matrix.toarray()[i, j]:.4f}")
import math
from collections import Counter

# Get vocabulary from preprocessed documents
vocab = set()
for doc in preprocessed_docs:
    vocab.update(doc.split())

# Compute Term Frequency (TF)
def compute_tf(doc):
    words = doc.split()
    word_counts = Counter(words)
    total_words = len(words)
    return {word: word_counts[word] / total_words for word in vocab}

tf_scores = [compute_tf(doc) for doc in preprocessed_docs]
# Compute Inverse Document Frequency (IDF)
def compute_idf(docs):
    num_docs = len(docs)
    idf_scores = {}

    for word in vocab:
        containing_docs = sum(1 for doc in docs if word in set(doc.split()))
        idf_scores[word] = math.log(num_docs / (1 + containing_docs))

    return idf_scores
# Compute TF-IDF
idf_scores = compute_idf(preprocessed_docs)
tfidf_scores = [{word: tf * idf_scores[word] for word, tf in tf.items()} for tf in tf_scores]

print("\nTF-IDF Scores:")
for i, doc_tfidf in enumerate(tfidf_scores):
    print(f"\nDocument {i+1}:")
    for word, score in doc_tfidf.items():
        print(f"\t{word}: {score:.4f}")
from sklearn.metrics.pairwise import cosine_similarity
def calculate_similarity(text1, text2):
    preprocessed_texts = [preprocess_text(text1), preprocess_text(text2)]

    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)

    similarity_score = cosine_similarity(tfidf_matrix[0], tfidf_matrix[1])[0][0]
    return similarity_score
def find_similar_words(text1, text2):
    words1 = set(preprocess_text(text1).split())
    words2 = set(preprocess_text(text2).split())

    common_words = words1.intersection(words2)
    return common_words
if __name__ == "__main__":
    text1 = "I enjoy eating apples"
    text2 = "Apples are sweet"
similarity_score = calculate_similarity(text1, text2)
print(f"Similarity Score: {similarity_score:.4f}")
similar_words = find_similar_words(text1, text2)
print(f"Common Words: {similar_words}")
